{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw-Nv6mZXIqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "from tensorflow import keras\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m04DgVGoXgon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data = (\n",
        "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
        "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
        "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
        "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
        "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
        "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
        "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
        "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
        "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
        "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
        "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
        "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
        "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
        "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
        "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
        "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
        "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
        "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
        "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
        "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhwFc2kAXz7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3S2eBeWX16R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_data_en,raw_data_fr=list(zip(*raw_data))\n",
        "raw_data_en,raw_data_fr=list(raw_data_en),list(raw_data_fr)\n",
        "\n",
        "train=[\"<start> \"+data+\" <end>\" for data in raw_data_en]\n",
        "target=[\"<start> \"+data+\" <end>\" for data in raw_data_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtEGdrIWYRhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tokenizer=keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "en_tokenizer.fit_on_texts(train)\n",
        "train=en_tokenizer.texts_to_sequences(train)\n",
        "train=keras.preprocessing.sequence.pad_sequences(train,padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvuMgLVvdmXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fr_tokenizer=keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "fr_tokenizer.fit_on_texts(target)\n",
        "target=fr_tokenizer.texts_to_sequences(target)\n",
        "target=keras.preprocessing.sequence.pad_sequences(target,padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94DDHjc6e1d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE=20\n",
        "BATCH_SIZE=5\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "dataset=tf.data.Dataset.from_tensor_slices((train,target))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "vocab_input_len=len(en_tokenizer.word_index)+1\n",
        "vocab_target_len=len(fr_tokenizer.word_index)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhIi9BDYficI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(keras.Model):\n",
        "  def __init__(self,vocal_size,embedding_size,enc_units,batch_size):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.enc_units=enc_units\n",
        "    self.embedding=keras.layers.Embedding(vocal_size,embedding_size)\n",
        "    self.gru=keras.layers.GRU(self.enc_units,\n",
        "                  return_sequences=True,\n",
        "                  return_state=True,\n",
        "                  recurrent_initializer=\"glorot_uniform\")\n",
        "    \n",
        "  def call(self,x,hidden):\n",
        "    x=self.embedding(x)\n",
        "    output,state=self.gru(x,initial_state=hidden)\n",
        "\n",
        "    return output,state\n",
        "  \n",
        "  def init_states(self):\n",
        "    return tf.zeros((self.batch_size,self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeRnrdWMjR04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder=Encoder(vocab_input_len, embedding_dim, units, BATCH_SIZE)\n",
        "sample_hidden=encoder.init_states()\n",
        "\n",
        "example_input_batch,example_target_batch=next(iter(dataset))\n",
        "sample_output,sample_hidden=encoder(example_input_batch,sample_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQyo3856OIH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(Attention,self).__init__()\n",
        "    self.W1=keras.layers.Dense(units)\n",
        "    self.W2=keras.layers.Dense(units)\n",
        "    self.V=keras.layers.Dense(1)\n",
        "\n",
        "  def call(self,query,enc_output):\n",
        "    hidden_with_time_axis=tf.expand_dims(query,1)\n",
        "    score=self.V(tf.nn.tanh(\n",
        "        self.W1(enc_output)+self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    attention_weights=tf.nn.softmax(score,axis=1)\n",
        "\n",
        "    context_vector=attention_weights*enc_output\n",
        "    context_vector=tf.reduce_sum(context_vector,axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSUBFgOvI-Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,dec_units,batch_size):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.dec_units=dec_units\n",
        "    self.embedding=keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    self.gru=keras.layers.GRU(self.dec_units,\n",
        "                 return_sequences=True,\n",
        "                 return_state=True,\n",
        "                 recurrent_initializer='glorot_uniform')\n",
        "    self.fc=keras.layers.Dense(vocab_size)\n",
        "    self.attention=Attention(self.dec_units)\n",
        "\n",
        "  def call(self,x,hidden,enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x=self.embedding(x)\n",
        "\n",
        "    x=tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)\n",
        "    \n",
        "    output,state=self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x=self.fc(output)\n",
        "\n",
        "    return x,state,attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O9fnoMUUagV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder=Decoder(vocab_target_len,embedding_dim,units,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KlPq_c1hVap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=keras.optimizers.Adam()\n",
        "loss_object=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=\"none\")\n",
        "\n",
        "def loss_function(real,pred):\n",
        "  mask=tf.math.logical_not(tf.math.equal(real,0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVSW3GykrZDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(input,target,enc_hidden):\n",
        "  loss=0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output,enc_hidden=encoder(input,enc_hidden)\n",
        "    dec_hidden=enc_hidden\n",
        "    dec_input=tf.expand_dims([fr_tokenizer.word_index[\"<start>\"]] * BATCH_SIZE,1)\n",
        "\n",
        "    # 教师强制 - 将目标词作为下一个输入\n",
        "    for t in range(1, target.shape[1]):\n",
        "      # 将编码器输出 （enc_output） 传送至解码器\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "      # 使用教师强制\n",
        "      dec_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pthXoZUOzTjX",
        "colab_type": "code",
        "outputId": "5446cd2d-45a7-4f6c-bbd6-afccfcc412eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS=50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start=time.time()\n",
        "\n",
        "  enc_hidden=encoder.init_states()\n",
        "  total_loss=0\n",
        "\n",
        "  for (batch,(input,target)) in enumerate(dataset.take(5)):\n",
        "    batch_loss=train_step(input,target,enc_hidden)\n",
        "    total_loss+=batch_loss\n",
        "  \n",
        "    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
        "  \n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.1386\n",
            "Epoch 1 Batch 1 Loss 3.5275\n",
            "Epoch 1 Batch 2 Loss 3.8363\n",
            "Epoch 1 Batch 3 Loss 2.9638\n",
            "Time taken for 1 epoch 16.38884973526001 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.1315\n",
            "Epoch 2 Batch 1 Loss 3.8064\n",
            "Epoch 2 Batch 2 Loss 3.2061\n",
            "Epoch 2 Batch 3 Loss 3.1021\n",
            "Time taken for 1 epoch 4.554631233215332 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.2553\n",
            "Epoch 3 Batch 1 Loss 3.4383\n",
            "Epoch 3 Batch 2 Loss 3.2938\n",
            "Epoch 3 Batch 3 Loss 2.8412\n",
            "Time taken for 1 epoch 4.643236398696899 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.2268\n",
            "Epoch 4 Batch 1 Loss 3.0467\n",
            "Epoch 4 Batch 2 Loss 3.0070\n",
            "Epoch 4 Batch 3 Loss 3.4539\n",
            "Time taken for 1 epoch 4.601182699203491 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.5145\n",
            "Epoch 5 Batch 1 Loss 2.7776\n",
            "Epoch 5 Batch 2 Loss 3.0394\n",
            "Epoch 5 Batch 3 Loss 2.9858\n",
            "Time taken for 1 epoch 4.619771957397461 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.1777\n",
            "Epoch 6 Batch 1 Loss 2.6194\n",
            "Epoch 6 Batch 2 Loss 2.7143\n",
            "Epoch 6 Batch 3 Loss 3.3713\n",
            "Time taken for 1 epoch 4.574537754058838 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.3614\n",
            "Epoch 7 Batch 1 Loss 3.3577\n",
            "Epoch 7 Batch 2 Loss 2.8877\n",
            "Epoch 7 Batch 3 Loss 2.8991\n",
            "Time taken for 1 epoch 4.614110231399536 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.4236\n",
            "Epoch 8 Batch 1 Loss 3.0391\n",
            "Epoch 8 Batch 2 Loss 3.0772\n",
            "Epoch 8 Batch 3 Loss 2.6074\n",
            "Time taken for 1 epoch 4.657308101654053 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5121\n",
            "Epoch 9 Batch 1 Loss 2.6384\n",
            "Epoch 9 Batch 2 Loss 2.7916\n",
            "Epoch 9 Batch 3 Loss 2.7066\n",
            "Time taken for 1 epoch 4.63325047492981 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.4885\n",
            "Epoch 10 Batch 1 Loss 2.2559\n",
            "Epoch 10 Batch 2 Loss 2.5857\n",
            "Epoch 10 Batch 3 Loss 2.6393\n",
            "Time taken for 1 epoch 4.59498929977417 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.3117\n",
            "Epoch 11 Batch 1 Loss 2.3440\n",
            "Epoch 11 Batch 2 Loss 2.2797\n",
            "Epoch 11 Batch 3 Loss 2.2738\n",
            "Time taken for 1 epoch 4.564332485198975 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.0779\n",
            "Epoch 12 Batch 1 Loss 2.1667\n",
            "Epoch 12 Batch 2 Loss 2.0964\n",
            "Epoch 12 Batch 3 Loss 1.9215\n",
            "Time taken for 1 epoch 4.640879392623901 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9617\n",
            "Epoch 13 Batch 1 Loss 1.8528\n",
            "Epoch 13 Batch 2 Loss 1.7918\n",
            "Epoch 13 Batch 3 Loss 1.5477\n",
            "Time taken for 1 epoch 4.5984883308410645 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.3606\n",
            "Epoch 14 Batch 1 Loss 1.5135\n",
            "Epoch 14 Batch 2 Loss 1.7652\n",
            "Epoch 14 Batch 3 Loss 1.6910\n",
            "Time taken for 1 epoch 4.708807706832886 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.3266\n",
            "Epoch 15 Batch 1 Loss 1.3015\n",
            "Epoch 15 Batch 2 Loss 1.3567\n",
            "Epoch 15 Batch 3 Loss 1.3355\n",
            "Time taken for 1 epoch 4.6650004386901855 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.3018\n",
            "Epoch 16 Batch 1 Loss 1.1167\n",
            "Epoch 16 Batch 2 Loss 1.1702\n",
            "Epoch 16 Batch 3 Loss 0.9995\n",
            "Time taken for 1 epoch 4.534806728363037 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.8079\n",
            "Epoch 17 Batch 1 Loss 0.9963\n",
            "Epoch 17 Batch 2 Loss 0.8037\n",
            "Epoch 17 Batch 3 Loss 0.8388\n",
            "Time taken for 1 epoch 4.6082940101623535 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6729\n",
            "Epoch 18 Batch 1 Loss 0.7598\n",
            "Epoch 18 Batch 2 Loss 0.7500\n",
            "Epoch 18 Batch 3 Loss 0.5306\n",
            "Time taken for 1 epoch 4.5762763023376465 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.6675\n",
            "Epoch 19 Batch 1 Loss 0.4575\n",
            "Epoch 19 Batch 2 Loss 0.4843\n",
            "Epoch 19 Batch 3 Loss 0.4443\n",
            "Time taken for 1 epoch 4.626674652099609 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3826\n",
            "Epoch 20 Batch 1 Loss 0.4163\n",
            "Epoch 20 Batch 2 Loss 0.2960\n",
            "Epoch 20 Batch 3 Loss 0.5773\n",
            "Time taken for 1 epoch 4.541358709335327 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.2363\n",
            "Epoch 21 Batch 1 Loss 0.3522\n",
            "Epoch 21 Batch 2 Loss 0.3058\n",
            "Epoch 21 Batch 3 Loss 0.3839\n",
            "Time taken for 1 epoch 4.6157777309417725 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2417\n",
            "Epoch 22 Batch 1 Loss 0.2284\n",
            "Epoch 22 Batch 2 Loss 0.1957\n",
            "Epoch 22 Batch 3 Loss 0.2074\n",
            "Time taken for 1 epoch 4.5166015625 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1951\n",
            "Epoch 23 Batch 1 Loss 0.1459\n",
            "Epoch 23 Batch 2 Loss 0.1930\n",
            "Epoch 23 Batch 3 Loss 0.1905\n",
            "Time taken for 1 epoch 4.46464467048645 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1810\n",
            "Epoch 24 Batch 1 Loss 0.1409\n",
            "Epoch 24 Batch 2 Loss 0.1283\n",
            "Epoch 24 Batch 3 Loss 0.1629\n",
            "Time taken for 1 epoch 4.511396169662476 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1435\n",
            "Epoch 25 Batch 1 Loss 0.1120\n",
            "Epoch 25 Batch 2 Loss 0.0705\n",
            "Epoch 25 Batch 3 Loss 0.1144\n",
            "Time taken for 1 epoch 4.4641053676605225 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0939\n",
            "Epoch 26 Batch 1 Loss 0.0754\n",
            "Epoch 26 Batch 2 Loss 0.0896\n",
            "Epoch 26 Batch 3 Loss 0.0932\n",
            "Time taken for 1 epoch 4.480377197265625 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0700\n",
            "Epoch 27 Batch 1 Loss 0.0605\n",
            "Epoch 27 Batch 2 Loss 0.0645\n",
            "Epoch 27 Batch 3 Loss 0.0630\n",
            "Time taken for 1 epoch 4.46607518196106 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0424\n",
            "Epoch 28 Batch 1 Loss 0.0485\n",
            "Epoch 28 Batch 2 Loss 0.0483\n",
            "Epoch 28 Batch 3 Loss 0.0572\n",
            "Time taken for 1 epoch 4.495368003845215 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0446\n",
            "Epoch 29 Batch 1 Loss 0.0477\n",
            "Epoch 29 Batch 2 Loss 0.0499\n",
            "Epoch 29 Batch 3 Loss 0.0418\n",
            "Time taken for 1 epoch 4.540428400039673 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0402\n",
            "Epoch 30 Batch 1 Loss 0.0337\n",
            "Epoch 30 Batch 2 Loss 0.0289\n",
            "Epoch 30 Batch 3 Loss 0.0424\n",
            "Time taken for 1 epoch 4.583674907684326 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0306\n",
            "Epoch 31 Batch 1 Loss 0.0316\n",
            "Epoch 31 Batch 2 Loss 0.0359\n",
            "Epoch 31 Batch 3 Loss 0.0337\n",
            "Time taken for 1 epoch 4.5696234703063965 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0293\n",
            "Epoch 32 Batch 1 Loss 0.0247\n",
            "Epoch 32 Batch 2 Loss 0.0232\n",
            "Epoch 32 Batch 3 Loss 0.0293\n",
            "Time taken for 1 epoch 4.509031057357788 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0240\n",
            "Epoch 33 Batch 1 Loss 0.0239\n",
            "Epoch 33 Batch 2 Loss 0.0257\n",
            "Epoch 33 Batch 3 Loss 0.0227\n",
            "Time taken for 1 epoch 4.501867055892944 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0202\n",
            "Epoch 34 Batch 1 Loss 0.0195\n",
            "Epoch 34 Batch 2 Loss 0.0257\n",
            "Epoch 34 Batch 3 Loss 0.0223\n",
            "Time taken for 1 epoch 4.489981412887573 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0177\n",
            "Epoch 35 Batch 1 Loss 0.0191\n",
            "Epoch 35 Batch 2 Loss 0.0167\n",
            "Epoch 35 Batch 3 Loss 0.0236\n",
            "Time taken for 1 epoch 4.493513584136963 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0228\n",
            "Epoch 36 Batch 1 Loss 0.0174\n",
            "Epoch 36 Batch 2 Loss 0.0147\n",
            "Epoch 36 Batch 3 Loss 0.0172\n",
            "Time taken for 1 epoch 4.560135364532471 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0167\n",
            "Epoch 37 Batch 1 Loss 0.0177\n",
            "Epoch 37 Batch 2 Loss 0.0149\n",
            "Epoch 37 Batch 3 Loss 0.0163\n",
            "Time taken for 1 epoch 4.4968955516815186 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0135\n",
            "Epoch 38 Batch 1 Loss 0.0162\n",
            "Epoch 38 Batch 2 Loss 0.0160\n",
            "Epoch 38 Batch 3 Loss 0.0151\n",
            "Time taken for 1 epoch 4.55275821685791 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0150\n",
            "Epoch 39 Batch 1 Loss 0.0143\n",
            "Epoch 39 Batch 2 Loss 0.0131\n",
            "Epoch 39 Batch 3 Loss 0.0147\n",
            "Time taken for 1 epoch 4.463695287704468 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0116\n",
            "Epoch 40 Batch 1 Loss 0.0161\n",
            "Epoch 40 Batch 2 Loss 0.0137\n",
            "Epoch 40 Batch 3 Loss 0.0122\n",
            "Time taken for 1 epoch 4.460549831390381 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0132\n",
            "Epoch 41 Batch 1 Loss 0.0130\n",
            "Epoch 41 Batch 2 Loss 0.0111\n",
            "Epoch 41 Batch 3 Loss 0.0133\n",
            "Time taken for 1 epoch 4.53965425491333 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0125\n",
            "Epoch 42 Batch 1 Loss 0.0109\n",
            "Epoch 42 Batch 2 Loss 0.0114\n",
            "Epoch 42 Batch 3 Loss 0.0131\n",
            "Time taken for 1 epoch 4.492701292037964 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0111\n",
            "Epoch 43 Batch 1 Loss 0.0106\n",
            "Epoch 43 Batch 2 Loss 0.0119\n",
            "Epoch 43 Batch 3 Loss 0.0119\n",
            "Time taken for 1 epoch 4.507007837295532 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0113\n",
            "Epoch 44 Batch 1 Loss 0.0102\n",
            "Epoch 44 Batch 2 Loss 0.0114\n",
            "Epoch 44 Batch 3 Loss 0.0104\n",
            "Time taken for 1 epoch 4.527873754501343 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0115\n",
            "Epoch 45 Batch 1 Loss 0.0092\n",
            "Epoch 45 Batch 2 Loss 0.0098\n",
            "Epoch 45 Batch 3 Loss 0.0108\n",
            "Time taken for 1 epoch 4.548571586608887 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0096\n",
            "Epoch 46 Batch 1 Loss 0.0093\n",
            "Epoch 46 Batch 2 Loss 0.0105\n",
            "Epoch 46 Batch 3 Loss 0.0100\n",
            "Time taken for 1 epoch 4.501971006393433 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0104\n",
            "Epoch 47 Batch 1 Loss 0.0095\n",
            "Epoch 47 Batch 2 Loss 0.0081\n",
            "Epoch 47 Batch 3 Loss 0.0097\n",
            "Time taken for 1 epoch 4.538795709609985 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0091\n",
            "Epoch 48 Batch 1 Loss 0.0094\n",
            "Epoch 48 Batch 2 Loss 0.0090\n",
            "Epoch 48 Batch 3 Loss 0.0088\n",
            "Time taken for 1 epoch 4.592539310455322 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0077\n",
            "Epoch 49 Batch 1 Loss 0.0076\n",
            "Epoch 49 Batch 2 Loss 0.0109\n",
            "Epoch 49 Batch 3 Loss 0.0085\n",
            "Time taken for 1 epoch 4.57059121131897 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0090\n",
            "Epoch 50 Batch 1 Loss 0.0084\n",
            "Epoch 50 Batch 2 Loss 0.0077\n",
            "Epoch 50 Batch 3 Loss 0.0082\n",
            "Time taken for 1 epoch 4.583706617355347 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ur5n3t0wJ_o",
        "colab_type": "code",
        "outputId": "aa4da873-652a-41cd-d348-c7c8e06543c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "attention_plot=np.zeros((target.shape[1],train.shape[0]))\n",
        "hidden=[tf.zeros((1,units))]\n",
        "enc_out,enc_hidden=encoder(tf.expand_dims(train[0,],0),hidden)\n",
        "\n",
        "dec_hidden=enc_hidden\n",
        "dec_input=tf.expand_dims([fr_tokenizer.word_index[\"<start>\"]],0)\n",
        "result=\"\"\n",
        "\n",
        "for t in range(target.shape[1]):\n",
        "  predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
        "  # 存储注意力权重以便后面制图\n",
        "\n",
        "  predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  if predicted_id != 0:\n",
        "    result += fr_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if fr_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      print(result)\n",
        "      break\n",
        "\n",
        "  # 预测的 ID 被输送回模型\n",
        "  dec_input = tf.expand_dims([predicted_id], 0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quel concept ridicule ! <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjmsn1_NbgOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}